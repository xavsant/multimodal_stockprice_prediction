{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-07</td>\n",
       "      <td>want to work at amazon apple or mckinsey some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-14</td>\n",
       "      <td>get recommendations from new york times report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-13</td>\n",
       "      <td>the business unit will partner with companies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>with superstars first in line apple appears to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>in an industry that avoids controversy the hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pub_date                                           abstract\n",
       "0  2015-04-07  want to work at amazon apple or mckinsey some ...\n",
       "1  2015-04-14  get recommendations from new york times report...\n",
       "2  2015-04-13  the business unit will partner with companies ...\n",
       "3  2015-04-22  with superstars first in line apple appears to...\n",
       "4  2015-04-01  in an industry that avoids controversy the hea..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#>=50% of annotators agreed on the financial sentiment \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = r'C:/Users/Tammy/Documents/GitHub/multimodal_stockprice_prediction/data/clean/stock_text_data/Apple_Inc_text_data.csv'\n",
    "# Load the CSV file with the specified encoding and column names\n",
    "df = pd.read_csv(file_path,encoding='utf-8')\n",
    "\n",
    "# Cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and special characters (except spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "# Check the first few rows of the data\n",
    "df.head()\n",
    "#reformat date\n",
    "# format datetime again\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date']) \n",
    "df['pub_date'] = df['pub_date'].dt.date\n",
    "#cleaning\n",
    "# df['headline'] = df['headline'].apply(clean_text) # change this line for abstract\n",
    "df['abstract'] = df['abstract'].apply(clean_text)\n",
    "# Select only the 'pub_date' and 'headline' columns\n",
    "df_new = df[['pub_date', 'abstract']] #change this line for abstract\n",
    "  \n",
    "# Check the new DataFrame\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class SentimentTrendPrediction(BaseModel):\n",
    "    SentimentScore: float\n",
    "    TrendStrength: float    # Predicted price as a string (could be float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up Gemini\n",
    "#set up gemini to process textual templates\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "load_dotenv()\n",
    "# Load environment variables from the .env file\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "client = genai.Client(api_key=API_KEY)#insert api key\n",
    "## Function to call Gemini API\n",
    "from google.genai import types\n",
    "import json\n",
    "\n",
    "# Function to get sentiment based on model output\n",
    "def gemini_predict_sentiment_trend(prompt, return_json=False):\n",
    "    try:\n",
    "        # Generate content from the model\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash-lite', \n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "            system_instruction= f\"\"\"\n",
    "        \n",
    "            You are a financial analyst specializing in quantitative market sentiment and trend analysis. Analyze each headline and provide TWO precise metrics:\n",
    "\n",
    "            1. SENTIMENT SCORE (-1.0 to 1.0):\n",
    "            • -1.0: Catastrophic news (bankruptcy, massive fraud, severe regulatory action)\n",
    "            • -0.8: Very negative news (significant earnings miss, major layoffs, lawsuits)\n",
    "            • -0.5: Moderately negative news (missed targets, restructuring, competitive challenges)\n",
    "            • -0.2: Slightly negative news (minor setbacks, cautious outlook)\n",
    "            • 0.0: Neutral or balanced news (mixed results, status quo maintained)\n",
    "            • 0.2: Slightly positive news (minor wins, modest growth)\n",
    "            • 0.5: Moderately positive news (good earnings, new partnerships, expanding markets)\n",
    "            • 0.8: Very positive news (exceeding expectations, major contracts, innovations)\n",
    "            • 1.0: Transformational news (breakthrough products, industry-changing acquisitions)\n",
    "\n",
    "            2. TREND STRENGTH (-1.0 to 1.0):\n",
    "            • Consider: Is this likely to create a MARKET MOVEMENT (price change)?\n",
    "            • -1.0: Strong, sustained downward pressure (systemic issues, prolonged negative impact)\n",
    "            • -0.6: Significant downward momentum (clear negative catalysts)\n",
    "            • -0.3: Slight downward pressure (minor concerns that may affect sentiment)\n",
    "            • 0.0: Neutral impact (balanced factors, unlikely to drive directional movement)\n",
    "            • 0.3: Slight upward potential (positive but limited catalyst)\n",
    "            • 0.6: Significant upward momentum (clear positive catalysts)\n",
    "            • 1.0: Strong, sustained upward potential (transformative positive developments)\n",
    "\n",
    "            CRITICAL INSTRUCTIONS:\n",
    "            - Differentiate between SENTIMENT (how positive/negative the news is) and TREND (likely market movement)\n",
    "            - Consider sector-specific implications - news impacts different industries differently\n",
    "            - Evaluate the scale of impact relative to company/market size\n",
    "            - Distinguish between short-term reactions vs. fundamental changes\n",
    "            - For ambiguous headlines, default to more moderate scores (-0.5 to +0.5 range)\n",
    "            -  Return ONLY the two values in JSON format, e.g., {{\"SentimentScore\": 0.5, \"TrendStrength\": -0.4}}\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    ",\n",
    "            max_output_tokens=75,  # Label only\n",
    "            temperature=0.5,      # More flexibility\n",
    "            top_k=5,              # Limit to top 5 choices\n",
    "            top_p=0.7,            # Consider tokens covering 70% probability mass\n",
    "            response_mime_type='application/json',\n",
    "            response_schema=SentimentTrendPrediction\n",
    "          # No stop sequence to avoid premature stops\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Check if the response is valid\n",
    "        response_text = response.text.strip()\n",
    "        if not response_text:\n",
    "            print(\"Error: No response text found.\")\n",
    "            return 'Error' if not return_json else {}\n",
    "\n",
    "        # Try parsing the response as JSON\n",
    "        try:\n",
    "            response_json = json.loads(response_text)\n",
    "            if return_json:\n",
    "                return response_json\n",
    "\n",
    "            return response_json\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing JSON. Response: {response_text}\")\n",
    "            return 'Error' if not return_json else {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return 'Error' if not return_json else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment with few-shot prompting\n",
    "def analyze_sentiment_trend(text): #alter prompt for abstract\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this financial headline and provide two metrics: \n",
    "    1. Sentiment Score: from -1.0 (extremely negative) to 1.0 (extremely positive)\n",
    "    2. Trend Strength: from -1.0 (strong downward trend) to 1.0 (strong upward trend)\n",
    "\n",
    "    Return results in JSON format like:\n",
    "    {{ \"SentimentScore\": X.X, \"TrendStrength\": Y.Y }}\n",
    "\n",
    "    Here are some examples with SentimentScore and TrendStrength:\n",
    "\n",
    "    Example 1: \n",
    "    Headline: \"Company Close to Finalizing Its 40 billion dollar funding.\" \n",
    "    {{ \"SentimentScore\": 0.9, \"TrendStrength\": 0.8 }}\n",
    "\n",
    "    Example 2: \n",
    "    Headline: \"Regulatory authorities block 10% of funds for key agency in US-China Tech Race.\"\n",
    "    {{ \"SentimentScore\": -0.8, \"TrendStrength\": -0.6 }}\n",
    "\n",
    "    Example 3: \n",
    "    Headline: \"Why Company B could be a key to a Company C's Deal.\"\n",
    "    {{ \"SentimentScore\": 0.2, \"TrendStrength\": 0.1 }}\n",
    "\n",
    "    Example 4: \n",
    "    Headline: \"Artificial intelligence boom might help mitigate some tariff pain.\"\n",
    "    {{ \"SentimentScore\": 0.3, \"TrendStrength\": 0.2 }}\n",
    "\n",
    "    Example 5: \n",
    "    Headline: \"Major banks face regulatory hurdles, impacting earnings outlook.\"\n",
    "    {{ \"SentimentScore\": -0.6, \"TrendStrength\": -0.5 }}\n",
    "\n",
    "    Example 6: \n",
    "    Headline: \"Company's $32 billion deal may signal a turning point for slow IPO, M&A markets.\"\n",
    "    {{ \"SentimentScore\": 0.8, \"TrendStrength\": 0.7 }}\n",
    "\n",
    "    Example 7: \n",
    "    Headline: \"Company X enters a strategic partnership with Company Y to expand its operations in Asia.\"\n",
    "    Sentiment Score: 0.7\n",
    "    {{ \"SentimentScore\": 0.7, \"TrendStrength\": 0.6 }}\n",
    "\n",
    "    Now, analyze this headline:\n",
    "\n",
    "    Headline: \"{text}\"\n",
    "    Response (in JSON format):\n",
    "   {{ \"SentimentScore\": , \"TrendStrength\": }}\"\"\"\n",
    "    \n",
    "    result = gemini_predict_sentiment_trend(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for news impact duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationPrediction(BaseModel):\n",
    "    PotentialImpactDays: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for duration prediction\n",
    "def gemini_predict_duration(prompt, return_json=False):\n",
    "    try:\n",
    "        # Generate content from the model\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash-lite', \n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=\"\"\"\n",
    "                You are a financial market expert specializing in assessing the impact duration of financial news. Your task is to:\n",
    "\n",
    "                ESTIMATE POTENTIAL IMPACT DAYS: Predict how many days (1-30) this headline's impact might last in the market.\n",
    "                   Consider: 1-3 days for short-lived news, 4-7 days for moderately impactful news, 8-14 days for significant developments,\n",
    "                   15-30 days for major structural changes or significant corporate events.\n",
    "\n",
    "                Consider the headline's significance, the entities involved, the type of event, and historical precedents for similar news.\n",
    "                Return ONLY the value in JSON format: {\"PotentialImpactDays\": Z}\n",
    "                \"\"\",\n",
    "                max_output_tokens=50,\n",
    "                temperature=0.5,\n",
    "                top_k=5,\n",
    "                top_p=0.7,\n",
    "                response_mime_type='application/json',\n",
    "                response_schema=DurationPrediction\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Check if the response is valid\n",
    "        response_text = response.text.strip()\n",
    "        if not response_text:\n",
    "            print(\"Error: No response text found.\")\n",
    "            return 'Error' if not return_json else {}\n",
    "\n",
    "        # Try parsing the response as JSON\n",
    "        try:\n",
    "            response_json = json.loads(response_text)\n",
    "            if return_json:\n",
    "                return response_json\n",
    "\n",
    "            return response_json\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing JSON. Response: {response_text}\")\n",
    "            return 'Error' if not return_json else {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return 'Error' if not return_json else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_duration(text):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this financial headline and estimate how many days (1-30) this news might impact the market.\n",
    "    Consider: \n",
    "    - 1-3 days for short-lived news\n",
    "    - 4-7 days for moderately impactful news \n",
    "    - 8-14 days for significant developments\n",
    "    - 15-30 days for major structural changes or significant corporate events\n",
    "\n",
    "    Return result in JSON format like:\n",
    "    {{ \"PotentialImpactDays\": Z }}\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Example 1: \n",
    "    Headline: \"Company close to finalizing its 40 billion dollar funding.\" \n",
    "    {{ \"PotentialImpactDays\": 14 }}\n",
    "\n",
    "    Example 2: \n",
    "    Headline: \"Regulatory authorities block 10% of funds for key agency in US-China Tech Race.\"\n",
    "    {{ \"PotentialImpactDays\": 7 }}\n",
    "\n",
    "    Example 3: \n",
    "    Headline: \"Why Company B could be a key to a Company C's Deal.\"\n",
    "    {{ \"PotentialImpactDays\": 3 }}\n",
    "\n",
    "    Example 4: \n",
    "    Headline: \"Artificial intelligence boom might help mitigate some tariff pain.\"\n",
    "    {{ \"PotentialImpactDays\": 5 }}\n",
    "\n",
    "    Example 5: \n",
    "    Headline: \"Major banks face regulatory hurdles, impacting earnings outlook.\"\n",
    "    {{ \"PotentialImpactDays\": 10 }}\n",
    "\n",
    "    Example 6: \n",
    "    Headline: \"Company's $32 billion deal may signal a turning point for slow IPO, M&A markets.\"\n",
    "    {{ \"PotentialImpactDays\": 21 }}\n",
    "\n",
    "    Now analyze this headline:\n",
    "    Headline: \"{text}\"\n",
    "    \"\"\"\n",
    "    \n",
    "    result = gemini_predict_duration(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentiment & Trend: 100%|██████████| 2143/2143 [25:53<00:00,  1.38headline/s]\n",
      "C:\\Users\\Tammy\\AppData\\Local\\Temp\\ipykernel_43964\\2059074659.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['gemini_sentiment'] = sentiment_list\n",
      "C:\\Users\\Tammy\\AppData\\Local\\Temp\\ipykernel_43964\\2059074659.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = sentiment_score_list\n",
      "C:\\Users\\Tammy\\AppData\\Local\\Temp\\ipykernel_43964\\2059074659.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['trend_strength'] = trend_strength_list\n",
      "Processing Duration: 100%|██████████| 2143/2143 [26:41<00:00,  1.34headline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pub_date                                           headline  \\\n",
      "0     2015-04-07     mba programs that get you where you want to go   \n",
      "1     2015-04-14                                  what were reading   \n",
      "2     2015-04-13  ibm creates watson health to analyze medical data   \n",
      "3     2015-04-22  whats that on beyoncs wrist let me guess an ap...   \n",
      "4     2015-04-01  daily report tech leaders come together to opp...   \n",
      "...          ...                                                ...   \n",
      "2138  2024-01-19  apple takes a humble approach to launching its...   \n",
      "2139  2024-01-25  apple overhauls app store in europe in respons...   \n",
      "2140  2024-01-31  the apple vision pro is a marvel but who will ...   \n",
      "2141  2024-01-05  us moves closer to filing sweeping antitrust c...   \n",
      "2142  2024-01-18                  charms can personalize your watch   \n",
      "\n",
      "         gemini_sentiment  sentiment_score  trend_strength  impact_days  \n",
      "0       Slightly Positive              0.2             0.1            3  \n",
      "1                 Neutral              0.0             0.0            1  \n",
      "2     Moderately Positive              0.7             0.6            7  \n",
      "3                 Neutral              0.0             0.0            1  \n",
      "4       Slightly Negative             -0.2            -0.1            7  \n",
      "...                   ...              ...             ...          ...  \n",
      "2138    Slightly Positive              0.2             0.1            5  \n",
      "2139    Slightly Positive              0.2             0.3           10  \n",
      "2140    Slightly Positive              0.2             0.1            5  \n",
      "2141        Very Negative             -0.8            -0.7           14  \n",
      "2142    Slightly Positive              0.2             0.1            3  \n",
      "\n",
      "[2143 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Tammy\\AppData\\Local\\Temp\\ipykernel_43964\\2059074659.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['impact_days'] = impact_days_list\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_headlines_with_separate_models(df, sentiment_column):\n",
    "    sentiment_list = []\n",
    "    sentiment_score_list = []\n",
    "    trend_strength_list = []\n",
    "    impact_days_list = []\n",
    "    \n",
    "    # Process with first model (sentiment and trend)\n",
    "    for idx, headline in tqdm(enumerate(df[sentiment_column]), total=len(df), desc=\"Processing Sentiment & Trend\", unit=\"headline\"):\n",
    "        # Get sentiment and trend predictions\n",
    "        result = analyze_sentiment_trend(headline)\n",
    "        \n",
    "        if isinstance(result, dict) and 'SentimentScore' in result:\n",
    "            sentiment_score = result['SentimentScore']\n",
    "            trend_strength = result['TrendStrength']\n",
    "            \n",
    "            # Store values\n",
    "            sentiment_score_list.append(sentiment_score)\n",
    "            trend_strength_list.append(trend_strength)\n",
    "            \n",
    "            # Determine categorical sentiment\n",
    "            if sentiment_score == 1.0:\n",
    "                sentiment = 'Extremely Positive'\n",
    "            elif sentiment_score >= 0.8:\n",
    "                sentiment = 'Very Positive'\n",
    "            elif sentiment_score >= 0.5:\n",
    "                sentiment = 'Moderately Positive'\n",
    "            elif sentiment_score >= 0.2:\n",
    "                sentiment = 'Slightly Positive'\n",
    "            elif sentiment_score == 0.0:\n",
    "                sentiment = 'Neutral'\n",
    "            elif sentiment_score >= -0.2:\n",
    "                sentiment = 'Slightly Negative'\n",
    "            elif sentiment_score >= -0.5:\n",
    "                sentiment = 'Moderately Negative'\n",
    "            elif sentiment_score >= -0.8:\n",
    "                sentiment = 'Very Negative'\n",
    "            else:\n",
    "                sentiment = 'Extremely Negative'\n",
    "            \n",
    "            sentiment_list.append(sentiment)\n",
    "        else:\n",
    "            # Handle error case\n",
    "            sentiment_list.append('Error')\n",
    "            sentiment_score_list.append(None)\n",
    "            trend_strength_list.append(None)\n",
    "    \n",
    "    # Add sentiment and trend to DataFrame\n",
    "    df['gemini_sentiment'] = sentiment_list\n",
    "    df['sentiment_score'] = sentiment_score_list\n",
    "    df['trend_strength'] = trend_strength_list\n",
    "    \n",
    "    # Process with second model (duration)\n",
    "    for idx, headline in tqdm(enumerate(df[sentiment_column]), total=len(df), desc=\"Processing Duration\", unit=\"headline\"):\n",
    "        # Get duration prediction\n",
    "        result = analyze_duration(headline)\n",
    "        \n",
    "        if isinstance(result, dict) and 'PotentialImpactDays' in result:\n",
    "            impact_days = result['PotentialImpactDays']\n",
    "            impact_days_list.append(impact_days)\n",
    "        else:\n",
    "            # Handle error case\n",
    "            impact_days_list.append(None)\n",
    "    \n",
    "    # Add duration to DataFrame\n",
    "    df['impact_days'] = impact_days_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the separate models\n",
    "df_analyzed = process_headlines_with_separate_models(df_new, 'headline')\n",
    "\n",
    "# Display results\n",
    "print(df_analyzed[['pub_date', 'headline', 'gemini_sentiment', 'sentiment_score', 'trend_strength', 'impact_days']])\n",
    "\n",
    "# Save to CSV\n",
    "df_analyzed.to_csv('gemini_abstract_features_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Headlines:   7%|▋         | 141/2143 [01:57<27:44,  1.20headline/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Apply to your DataFrame\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m df_new \u001b[38;5;241m=\u001b[39m apply_without_delay(df_new, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_new[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpub_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mapply_without_delay\u001b[1;34m(df, sentiment_column)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize tqdm progress bar\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, headline \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(df[sentiment_column]), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Headlines\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Get sentiment score directly using your modified find_sentiment_few_shot function\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     sentiment_score \u001b[38;5;241m=\u001b[39m find_sentiment_few_shot(headline)  \u001b[38;5;66;03m# This now returns the score\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Append the result\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     score_list\u001b[38;5;241m.\u001b[39mappend(sentiment_score)\n",
      "Cell \u001b[1;32mIn[4], line 125\u001b[0m, in \u001b[0;36mfind_sentiment_few_shot\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_sentiment_few_shot\u001b[39m(text):\n\u001b[0;32m     73\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124m    Analyze the sentiment of financial headlines on a scale from -1.0 to 1.0:\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m    -1.0 = Extremely negative\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124m    Response (in JSON format):\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 125\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m gemini_predict(prompt)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiment\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mgemini_predict\u001b[1;34m(prompt, return_json)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgemini_predict\u001b[39m(prompt, return_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Generate content from the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m     11\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-2.0-flash-lite\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     12\u001b[0m             contents\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m     13\u001b[0m             config\u001b[38;5;241m=\u001b[39mtypes\u001b[38;5;241m.\u001b[39mGenerateContentConfig(\n\u001b[0;32m     14\u001b[0m             system_instruction\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m            You are a financial analyst with expertise in sentiment analysis of financial news. Your task is to assess the sentiment of financial headlines precisely and provide a sentiment score on a scale from -1.0 (Extremely Negative) to 1.0 (Extremely Positive), considering market trends, economic events, corporate performance, and geopolitical issues. Your analysis should be nuanced, objective, and based on both tone and implications of the headline.\u001b[39m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m            IMPORTANT: Use the following sentiment scale:\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m            -1.0 = Extremely Negative, -0.8 = Very Negative, -0.5 = Moderately Negative, -0.2 = Slightly Negative, \u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m            0.0 = Neutral, 0.2 = Slightly Positive, 0.5 = Moderately Positive, 0.8 = Very Positive, 1.0 = Extremely Positive. \u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m            Ensure accuracy in assessing sentiment intensity, avoiding bias or oversimplification.\u001b[39m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m            Input Format: You will be given a financial headline. Assess the sentiment conveyed by the headline, considering potential market, corporate, or economic implications.\u001b[39m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m            When performing sentiment analysis:\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m            1. Analyze the headline’s explicit and implicit meanings, considering positive and negative indicators.\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m            2. Pay attention to words conveying confidence, uncertainty, success, failure, or risk.\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m            3. Ensure objectivity and base your analysis on the actual content and tone, not assumptions.\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124m            4. Return the sentiment score in JSON format, e.g., \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 0.5\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m based on the provided scale.\u001b[39m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m            Your analysis should be thorough, precise, and grounded in financial contexts like market sentiment, economic conditions, or corporate dynamics.\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     32\u001b[0m ,\n\u001b[0;32m     33\u001b[0m             max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,  \u001b[38;5;66;03m# Label only\u001b[39;00m\n\u001b[0;32m     34\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,      \u001b[38;5;66;03m# More flexibility\u001b[39;00m\n\u001b[0;32m     35\u001b[0m             top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,              \u001b[38;5;66;03m# Limit to top 5 choices\u001b[39;00m\n\u001b[0;32m     36\u001b[0m             top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,            \u001b[38;5;66;03m# Consider tokens covering 70% probability mass\u001b[39;00m\n\u001b[0;32m     37\u001b[0m             response_mime_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m             response_schema\u001b[38;5;241m=\u001b[39mScorePrediction\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;66;03m# No stop sequence to avoid premature stops\u001b[39;00m\n\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m         )\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# Check if the response is valid\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\google\\genai\\models.py:5304\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5302\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5303\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5304\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content(\n\u001b[0;32m   5305\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel, contents\u001b[38;5;241m=\u001b[39mcontents, config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m   5306\u001b[0m   )\n\u001b[0;32m   5307\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   5308\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\google\\genai\\models.py:4272\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   4269\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   4270\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 4272\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m   4273\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, path, request_dict, http_options\n\u001b[0;32m   4274\u001b[0m )\n\u001b[0;32m   4276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[0;32m   4277\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[0;32m   4278\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[0;32m   4279\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:575\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    567\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    570\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    571\u001b[0m ):\n\u001b[0;32m    572\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m    573\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m    574\u001b[0m   )\n\u001b[1;32m--> 575\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(http_request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    576\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[0;32m    577\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:488\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    484\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    485\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    486\u001b[0m   )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_unauthorized(http_request, stream)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:503\u001b[0m, in \u001b[0;36mBaseApiClient._request_unauthorized\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    500\u001b[0m     data \u001b[38;5;241m=\u001b[39m http_request\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    502\u001b[0m http_session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[1;32m--> 503\u001b[0m response \u001b[38;5;241m=\u001b[39m http_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    504\u001b[0m     method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    505\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    506\u001b[0m     headers\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    507\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    508\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    509\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    510\u001b[0m )\n\u001b[0;32m    511\u001b[0m errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    513\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    514\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def apply_without_delay(df, sentiment_column):\n",
    "    sentiment_list = []\n",
    "    score_list = []\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    for idx, headline in tqdm(enumerate(df[sentiment_column]), total=len(df), desc=\"Processing Headlines\", unit=\"headline\"):\n",
    "        \n",
    "        # Get sentiment score directly using your modified find_sentiment_few_shot function\n",
    "        sentiment_score = find_sentiment_few_shot(headline)  # This now returns the score\n",
    "        \n",
    "        # Append the result\n",
    "        score_list.append(sentiment_score)\n",
    "        \n",
    "        # Derive categorical sentiment from the score based on your custom scale\n",
    "        if sentiment_score == 1.0:\n",
    "            sentiment = 'Extremely Positive'\n",
    "        elif sentiment_score >= 0.8:\n",
    "            sentiment = 'Very Positive'\n",
    "        elif sentiment_score >= 0.5:\n",
    "            sentiment = 'Moderately Positive'\n",
    "        elif sentiment_score >= 0.2:\n",
    "            sentiment = 'Slightly Positive'\n",
    "        elif sentiment_score == 0.0:\n",
    "            sentiment = 'Neutral'\n",
    "        elif sentiment_score >= -0.2:\n",
    "            sentiment = 'Slightly Negative'\n",
    "        elif sentiment_score >= -0.5:\n",
    "            sentiment = 'Moderately Negative'\n",
    "        elif sentiment_score >= -0.8:\n",
    "            sentiment = 'Very Negative'\n",
    "        else:\n",
    "            sentiment = 'Extremely Negative'\n",
    "        \n",
    "        sentiment_list.append(sentiment)\n",
    "    \n",
    "    # Add Gemini's predictions while keeping original labels intact\n",
    "    df['gemini_sentiment'] = sentiment_list  # Adding the categorical sentiment label\n",
    "    df['sentiment_score'] = score_list  # Adding the sentiment score (numerical)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_new = apply_without_delay(df_new, 'headline')\n",
    "print(df_new[['pub_date', 'headline', 'gemini_sentiment', 'sentiment_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('gemini_sentiment_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_without_delay(df, sentiment_column):\n",
    "    sentiment_list = []\n",
    "    score_list = []\n",
    "    \n",
    "    for idx, headline in enumerate(df[sentiment_column]):\n",
    "        print(f\"Processing {idx + 1}/{len(df)}: {headline}\")  # Progress indicator\n",
    "        \n",
    "        # Get sentiment score directly using your modified find_sentiment_few_shot function\n",
    "        sentiment_score = find_sentiment_few_shot(headline)  # This now returns the score\n",
    "        \n",
    "        # Append the result\n",
    "        score_list.append(sentiment_score)\n",
    "        \n",
    "        # Derive categorical sentiment from the score based on your custom scale\n",
    "        if sentiment_score == 1.0:\n",
    "            sentiment = 'Extremely Positive'\n",
    "        elif sentiment_score >= 0.8:\n",
    "            sentiment = 'Very Positive'\n",
    "        elif sentiment_score >= 0.5:\n",
    "            sentiment = 'Moderately Positive'\n",
    "        elif sentiment_score >= 0.2:\n",
    "            sentiment = 'Slightly Positive'\n",
    "        elif sentiment_score == 0.0:\n",
    "            sentiment = 'Neutral'\n",
    "        elif sentiment_score >= -0.2:\n",
    "            sentiment = 'Slightly Negative'\n",
    "        elif sentiment_score >= -0.5:\n",
    "            sentiment = 'Moderately Negative'\n",
    "        elif sentiment_score >= -0.8:\n",
    "            sentiment = 'Very Negative'\n",
    "        else:\n",
    "            sentiment = 'Extremely Negative'\n",
    "        \n",
    "        sentiment_list.append(sentiment)\n",
    "    \n",
    "    # Add Gemini's predictions while keeping original labels intact\n",
    "    df['gemini_sentiment'] = sentiment_list  # Adding the categorical sentiment label\n",
    "    df['sentiment_score'] = score_list  # Adding the sentiment score (numerical)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_new = apply_without_delay(df_new, 'headline')\n",
    "print(df_new[['pub_date', 'headline', 'gemini_sentiment', 'sentiment_score']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
