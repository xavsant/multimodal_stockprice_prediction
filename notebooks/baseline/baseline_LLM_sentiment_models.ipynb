{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "(Only general setup, model specific imports are done within sections for the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General (modify where necessary)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/cleaned/Apple_Inc_text_data.csv')\n",
    "\n",
    "# format datetime again\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date']) \n",
    "df['pub_date'] = df['pub_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>lead_para</th>\n",
       "      <th>headline</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>section_name</th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>rank</th>\n",
       "      <th>web_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-07</td>\n",
       "      <td>Want to work at Amazon, Apple or McKinsey? Som...</td>\n",
       "      <td>With some 13,000 graduate schools of business ...</td>\n",
       "      <td>M.B.A. Programs That Get You Where You Want to Go</td>\n",
       "      <td>article</td>\n",
       "      <td>Education</td>\n",
       "      <td>News</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.nytimes.com/2015/04/12/education/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-14</td>\n",
       "      <td>Get recommendations from New York Times report...</td>\n",
       "      <td>Get recommendations from New York Times report...</td>\n",
       "      <td>What We’re Reading</td>\n",
       "      <td>article</td>\n",
       "      <td>Blogs</td>\n",
       "      <td>News</td>\n",
       "      <td>13</td>\n",
       "      <td>https://news.blogs.nytimes.com/2015/04/14/what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-13</td>\n",
       "      <td>The business unit will partner with companies ...</td>\n",
       "      <td>IBM is taking its Watson artificial-intelligen...</td>\n",
       "      <td>IBM Creates Watson Health to Analyze Medical Data</td>\n",
       "      <td>article</td>\n",
       "      <td>Technology</td>\n",
       "      <td>News</td>\n",
       "      <td>8</td>\n",
       "      <td>https://bits.blogs.nytimes.com/2015/04/13/ibm-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>With superstars first in line, Apple appears t...</td>\n",
       "      <td>Two weeks ago, Pharrell Williams posted an Ins...</td>\n",
       "      <td>What’s That on Beyoncé’s Wrist? Let Me Guess ....</td>\n",
       "      <td>article</td>\n",
       "      <td>Style</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.nytimes.com/2015/04/23/style/whats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>In an industry that avoids controversy, the he...</td>\n",
       "      <td>The technology industry’s leaders have found t...</td>\n",
       "      <td>Daily Report: Tech Leaders Come Together to Op...</td>\n",
       "      <td>article</td>\n",
       "      <td>Technology</td>\n",
       "      <td>News</td>\n",
       "      <td>3</td>\n",
       "      <td>https://bits.blogs.nytimes.com/2015/04/01/dail...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pub_date                                           abstract  \\\n",
       "0  2015-04-07  Want to work at Amazon, Apple or McKinsey? Som...   \n",
       "1  2015-04-14  Get recommendations from New York Times report...   \n",
       "2  2015-04-13  The business unit will partner with companies ...   \n",
       "3  2015-04-22  With superstars first in line, Apple appears t...   \n",
       "4  2015-04-01  In an industry that avoids controversy, the he...   \n",
       "\n",
       "                                           lead_para  \\\n",
       "0  With some 13,000 graduate schools of business ...   \n",
       "1  Get recommendations from New York Times report...   \n",
       "2  IBM is taking its Watson artificial-intelligen...   \n",
       "3  Two weeks ago, Pharrell Williams posted an Ins...   \n",
       "4  The technology industry’s leaders have found t...   \n",
       "\n",
       "                                            headline doc_type section_name  \\\n",
       "0  M.B.A. Programs That Get You Where You Want to Go  article    Education   \n",
       "1                                 What We’re Reading  article        Blogs   \n",
       "2  IBM Creates Watson Health to Analyze Medical Data  article   Technology   \n",
       "3  What’s That on Beyoncé’s Wrist? Let Me Guess ....  article        Style   \n",
       "4  Daily Report: Tech Leaders Come Together to Op...  article   Technology   \n",
       "\n",
       "  type_of_material  rank                                            web_url  \n",
       "0             News     7  https://www.nytimes.com/2015/04/12/education/e...  \n",
       "1             News    13  https://news.blogs.nytimes.com/2015/04/14/what...  \n",
       "2             News     8  https://bits.blogs.nytimes.com/2015/04/13/ibm-...  \n",
       "3             News     1  https://www.nytimes.com/2015/04/23/style/whats...  \n",
       "4             News     3  https://bits.blogs.nytimes.com/2015/04/01/dail...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy of df for sentiment analysis, may not actually be necessary, copied from previous nb\n",
    "sentiment_df=df.copy() #save the df first \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM: Gemini\n",
    "nerfed by api limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q \"google-genai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import os\n",
    "from google import genai\n",
    "# client = genai.Client(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>The Tech That Needs Fixing in 2024, and What G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>The True Price of Apple’s $3,500 Vision Pro Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>Why Making Face Computers Cool Isn’t Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>How to Cut Down Your Screen Time but Still Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>Apple to Offer Rare Discount on iPhones in China</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline\n",
       "2114  The Tech That Needs Fixing in 2024, and What G...\n",
       "2115  The True Price of Apple’s $3,500 Vision Pro Is...\n",
       "2116          Why Making Face Computers Cool Isn’t Easy\n",
       "2117  How to Cut Down Your Screen Time but Still Get...\n",
       "2118   Apple to Offer Rare Discount on iPhones in China"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = sentiment_df[['headline']].tail(10)\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               headline gemini_sentiment\n",
      "2114  The Tech That Needs Fixing in 2024, and What G...          Neutral\n",
      "2115  The True Price of Apple’s $3,500 Vision Pro Is...         Negative\n",
      "2116          Why Making Face Computers Cool Isn’t Easy         Negative\n",
      "2117  How to Cut Down Your Screen Time but Still Get...          Neutral\n",
      "2118   Apple to Offer Rare Discount on iPhones in China          Neutral\n",
      "2119  Apple Takes a Humble Approach to Launching Its...          Neutral\n",
      "2120  Apple Overhauls App Store in Europe, in Respon...          Neutral\n",
      "2121  The Apple Vision Pro Is a Marvel. But Who Will...          Neutral\n",
      "2122  U.S. Moves Closer to Filing Sweeping Antitrust...         Negative\n",
      "2123                  Charms Can Personalize Your Watch          Neutral\n"
     ]
    }
   ],
   "source": [
    "import google.genai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "# Initialize the client\n",
    "# client = genai.Client()\n",
    "\n",
    "# Function to get sentiment based on model output\n",
    "def gemini_predict(prompt):\n",
    "    # Generate content from the model\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash', \n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Assuming the model output is a sentiment label (e.g., Positive, Negative)\n",
    "    # You may need to adjust based on the actual format of the response\n",
    "    sentiment = response.text.strip()\n",
    "    \n",
    "    # Return the sentiment label, default to 'Neutral' if the model is uncertain\n",
    "    if sentiment not in ['Positive', 'Negative']:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Function to apply sentiment classification using the prompt for headlines\n",
    "def find_sentiment_zero_shot(text):\n",
    "    prompt = f\"\"\"Evaluate the sentiment conveyed by the headline with respect to Apple from an investment perspective. \n",
    "    Assign one of the following sentiment labels:\n",
    "    Positive: For headlines with positive implications.\n",
    "    Negative: For headlines with negative implications.\n",
    "    Neutral: For headlines with unclear or neutral implications.\n",
    "    \n",
    "    Headline: {text}\n",
    "    Sentiment: \"\"\"\n",
    "    \n",
    "    # Get sentiment from the model\n",
    "    sentiment = gemini_predict(prompt)\n",
    "    return sentiment\n",
    "\n",
    "def apply_with_delay(df, sentiment_column, delay=4):\n",
    "    sentiment_list = []\n",
    "    \n",
    "    for headline in df[sentiment_column]:\n",
    "        # Apply sentiment classification\n",
    "        sentiment = find_sentiment_zero_shot(headline)\n",
    "        sentiment_list.append(sentiment)\n",
    "        \n",
    "        # Delay to respect the RPM limit\n",
    "        time.sleep(delay)  # Delay in seconds (delay = 4 seconds to stay within 15 requests per minute)\n",
    "    \n",
    "    # Add the results to the dataframe\n",
    "    df['gemini_sentiment'] = sentiment_list\n",
    "    return df\n",
    "\n",
    "# Apply the function with delay to the sentiment DataFrame\n",
    "test_df = apply_with_delay(test, 'headline')\n",
    "\n",
    "# Display the results\n",
    "print(test_df[['headline', 'gemini_sentiment']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2124 entries, 0 to 2123\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   pub_date          2124 non-null   object\n",
      " 1   abstract          2124 non-null   object\n",
      " 2   lead_para         2124 non-null   object\n",
      " 3   headline          2124 non-null   object\n",
      " 4   doc_type          2124 non-null   object\n",
      " 5   section_name      2124 non-null   object\n",
      " 6   type_of_material  2124 non-null   object\n",
      " 7   rank              2124 non-null   int64 \n",
      " 8   web_url           2124 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 149.5+ KB\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM: Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\tammy\\anaconda3\\lib\\site-packages (2.6.0+cu126)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.3-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tammy\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached bitsandbytes-0.45.3-py3-none-win_amd64.whl (75.4 MB)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.4.0 bitsandbytes-0.45.3 huggingface-hub-0.29.2 peft-0.14.0 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "# !pip install transformers torch peft bitsandbytes accelerate sentencepiece blobfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load the model using BitsAndBytesConfig for quantization\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     21\u001b[0m     base_model, \n\u001b[0;32m     22\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Automatically distribute layers across available devices\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,  \u001b[38;5;66;03m# Use quantization configuration\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, peft_model)\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4262\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[0;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\Tammy\\anaconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast, BitsAndBytesConfig\n",
    "from peft import PeftModel  # 0.5.0\n",
    "import torch\n",
    "\n",
    "# Load Models\n",
    "base_model = \"NousResearch/Llama-2-13b-hf\" \n",
    "peft_model = \"FinGPT/fingpt-sentiment_llama2-13b_lora\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(base_model)\n",
    "\n",
    "# Set padding token if needed\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define the quantization configuration for 8-bit or 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)  # You can also use load_in_4bit=True for 4-bit\n",
    "\n",
    "# Load the model for causal language generation and the LoRA fine-tuned model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model using BitsAndBytesConfig for quantization\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    device_map=\"auto\",  # Automatically distribute layers across available devices\n",
    "    quantization_config=quantization_config,  # Use quantization configuration\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Make prompts\n",
    "prompt = [\n",
    "    '''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "    Input: FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs.\n",
    "    Answer: ''',\n",
    "    '''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "    Input: According to Gran, the company has no plans to move all production to Russia, although that is where the company is growing.\n",
    "    Answer: ''',\n",
    "    '''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "    Input: A tinyurl link takes users to a scamming site promising that users can earn thousands of dollars by becoming a Google (NASDAQ: GOOG) Cash advertiser.\n",
    "    Answer: ''',\n",
    "]\n",
    "\n",
    "# Tokenize the prompts\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move tensors to the correct device (GPU or CPU)\n",
    "tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "\n",
    "# Generate responses from the model\n",
    "with torch.no_grad():  # Disable gradients during inference\n",
    "    res = model.generate(**tokens, max_length=512)\n",
    "\n",
    "# Decode the results and extract the sentiment part\n",
    "res_sentences = [tokenizer.decode(i, skip_special_tokens=True) for i in res]\n",
    "\n",
    "# Extract the answer part (everything after 'Answer: ')\n",
    "out_text = [o.split(\"Answer: \")[1].strip() for o in res_sentences]\n",
    "\n",
    "# Show results\n",
    "for sentiment in out_text:\n",
    "    print(sentiment)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM: DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
